"""
LiveKit Agent with pluggable AI providers using LiveKit's native event system and state management.
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Optional, Dict, Any, List, AsyncIterable

from livekit import rtc
from livekit.agents import (
    Agent,
    AgentSession,
    AutoSubscribe, 
    JobContext, 
    WorkerOptions, 
    cli,
    JobProcess,
    JobRequest,
    RoomInputOptions,
    ModelSettings,
    FunctionTool,
    UserInputTranscribedEvent,
    ConversationItemAddedEvent,
    SpeechCreatedEvent,
    AgentStateChangedEvent,
    UserStateChangedEvent,
    ErrorEvent,
    VADEventTypes,
    Metrics
)
from livekit.agents import llm, stt
from livekit.agents.metrics import (
    Counter,
    Gauge,
    Histogram,
    LabelNames,
    Labels
)

from .providers import ProviderFactory
from .providers.base import (
    STTConfig, LLMConfig, TTSConfig,
    LLMMessage, ProviderType
)
from .config.settings import Settings
from .services.backend_client import BackendClient


logger = logging.getLogger(__name__)


class VoiceAssistantAgent(Agent):
    """
    Main voice assistant agent that orchestrates STT, LLM, and TTS providers.
    Uses LiveKit's native event system and state management.
    
    Inherits from LiveKit's Agent class to properly integrate with the LiveKit ecosystem.
    
    State is managed through:
    - LiveKit room.metadata for room-level state
    - LiveKit participant.metadata for user context
    - In-memory conversation history for current session
    - Backend API for persistent storage
    """

    def _setup_metrics(self):
        """Initialize all metrics collectors."""
        # Create metrics namespace
        namespace = "voice_assistant"
        
        # STT Metrics
        self.metrics.stt_latency = Histogram(
            name=f"{namespace}_stt_latency_seconds",
            help="Speech-to-text transcription latency in seconds"
        )
        self.metrics.stt_confidence = Histogram(
            name=f"{namespace}_stt_confidence",
            help="Speech-to-text confidence scores",
            buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        )
        self.metrics.stt_interim_results = Counter(
            name=f"{namespace}_stt_interim_results_total",
            help="Total number of interim STT results"
        )
        self.metrics.stt_final_results = Counter(
            name=f"{namespace}_stt_final_results_total",
            help="Total number of final STT results"
        )
        self.metrics.stt_word_count = Counter(
            name=f"{namespace}_stt_words_total",
            help="Total number of words transcribed"
        )
        
        # LLM Metrics
        self.metrics.llm_latency = Histogram(
            name=f"{namespace}_llm_latency_seconds",
            help="LLM response generation latency in seconds"
        )
        self.metrics.llm_tokens_generated = Counter(
            name=f"{namespace}_llm_tokens_generated_total",
            help="Total number of tokens generated by LLM"
        )
        self.metrics.llm_prompt_tokens = Counter(
            name=f"{namespace}_llm_prompt_tokens_total",
            help="Total number of prompt tokens processed by LLM"
        )
        
        # TTS Metrics
        self.metrics.tts_latency = Histogram(
            name=f"{namespace}_tts_latency_seconds",
            help="Text-to-speech synthesis latency in seconds"
        )
        self.metrics.tts_requests = Counter(
            name=f"{namespace}_tts_requests_total",
            help="Total number of TTS synthesis requests"
        )
        self.metrics.tts_characters = Counter(
            name=f"{namespace}_tts_characters_total",
            help="Total number of characters synthesized"
        )
        self.metrics.tts_audio_duration = Histogram(
            name=f"{namespace}_tts_audio_duration_seconds",
            help="Duration of synthesized audio in seconds"
        )
        
        # Error Metrics
        self.metrics.errors = Counter(
            name=f"{namespace}_errors_total",
            help="Total number of errors by type",
            label_names=["source", "recoverable"]
        )
        
        # Session Metrics
        self.metrics.session_duration = Gauge(
            name=f"{namespace}_session_duration_seconds",
            help="Current session duration in seconds"
        )
        self.metrics.active_participants = Gauge(
            name=f"{namespace}_active_participants",
            help="Number of active participants in the session"
        )
        
    def __init__(
        self,
        settings: Settings,
        backend_client: BackendClient,
        stt_config: Optional[STTConfig] = None,
        llm_config: Optional[LLMConfig] = None,
        tts_config: Optional[TTSConfig] = None,
        instructions: str = "You are a helpful voice AI assistant"
    ):
        super().__init__(instructions=instructions)
        self.settings = settings
        self.backend_client = backend_client
        self.stt_config = stt_config
        self.llm_config = llm_config
        self.tts_config = tts_config
        
        # Initialize metrics
        self._setup_metrics()
        
        # Session state
        self.conversation_history: List[LLMMessage] = []
        self.session_start = datetime.utcnow()
        self.conversation_id: Optional[str] = None
        self.participant_states: Dict[str, Dict[str, Any]] = {}
        
        # Performance tracking
        self.total_stt_latency = 0.0
        self.total_llm_latency = 0.0
        self.total_tts_latency = 0.0
        self.request_count = 0
        
        # Providers will be initialized in on_session_start
        self.stt_provider = None
        self.llm_provider = None 
        self.tts_provider = None
        self._audio_source: Optional[rtc.AudioSource] = None

    async def on_enter(self) -> None:
        """Called when the agent becomes active in a session."""
        logger.info("Agent becoming active in session")
        
        # Initialize providers if configs provided, otherwise use session defaults
        if self.stt_config:
            self.stt_provider = ProviderFactory.create_stt_provider(self.stt_config)
            await self.stt_provider.initialize()
        
        if self.llm_config:
            self.llm_provider = ProviderFactory.create_llm_provider(self.llm_config)
            await self.llm_provider.initialize()
            
        if self.tts_config:
            self.tts_provider = ProviderFactory.create_tts_provider(self.tts_config)
            await self.tts_provider.initialize()
            
        # Set up LiveKit event handlers in the session
        self.session.on("user_input_transcribed", self._handle_transcription)
        self.session.on("conversation_item_added", self._handle_conversation_item)
        self.session.on("speech_created", self._handle_speech_created)
        self.session.on("agent_state_changed", self._handle_agent_state)
        self.session.on("user_state_changed", self._handle_user_state)
        self.session.on("error", self._handle_error)
        
        # Load context from room metadata
        if self.session.room.metadata:
            try:
                room_data = json.loads(self.session.room.metadata)
                self.conversation_id = room_data.get("conversation_id")
            except json.JSONDecodeError:
                logger.error("Failed to parse room metadata")
        
        # Greet the user with a warm welcome
        await self.session.generate_reply(
            instructions="Greet the user with a warm welcome"
        )
        
        # Load recent conversation history from backend if available
        if self.conversation_id and self.backend_client:
            recent_messages = await self.backend_client.get_recent_messages(
                conversation_id=self.conversation_id,
                limit=10
            )
            if recent_messages:
                self.conversation_history.extend(recent_messages)
                logger.info(f"Loaded {len(recent_messages)} recent messages from backend")

    async def on_exit(self) -> None:
        """Called before the agent gives control to another agent."""
        # Say goodbye before exiting
        await self.session.generate_reply(
            instructions="Tell the user a friendly goodbye before you exit."
        )
        
        # Save conversation state if needed
        if self.backend_client and self.conversation_id:
            await self.backend_client.save_conversation_state(
                conversation_id=self.conversation_id,
                messages=self.conversation_history
            )

    async def on_user_turn_completed(
        self,
        turn_ctx: llm.ChatContext,
        new_message: llm.ChatMessage
    ) -> None:
        """
        Called when the user's turn has ended, before the agent's reply.
        This is where we can modify the user's message or add context before processing.
        """
        # Add the message to our conversation history
        self.conversation_history.append(
            LLMMessage(role="user", content=new_message.text_content)
        )
        
        # Optional: Modify message content if needed
        # Example: Filter out offensive content or add context

        # new_message.content = filter_offensive_content(new_message.content)
        
        # Add any relevant context from our backend
        if self.backend_client:
            context = await self.backend_client.get_relevant_context(
                new_message.text_content
            )
            if context:
                turn_ctx.add_message(
                    role="assistant",
                    content=f"Additional context: {context}"
                )
                
    async def _handle_transcription(self, event: "UserInputTranscribedEvent") -> None:
        """Handle STT transcription events."""
        if not event.is_final:
            # Update interim metrics
            self.metrics.stt_interim_results.inc()
            return

        if not self.conversation_id:
            return

        # Update metrics for final transcription
        self.metrics.stt_final_results.inc()
        self.metrics.stt_word_count.inc(len(event.transcript.split()))
        
        if hasattr(event, "latency"):
            self.metrics.stt_latency.observe(event.latency)
            
        if hasattr(event, "confidence"):
            self.metrics.stt_confidence.observe(event.confidence)

        # Store transcription in database
        asyncio.create_task(
            self.backend_client.store_transcription(
                conversation_id=self.conversation_id,
                text=event.transcript,
                metadata={
                    "language": event.language,
                    "speaker_id": event.speaker_id,
                    "is_final": event.is_final,
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
        )

    async def _handle_conversation_item(self, event: "ConversationItemAddedEvent") -> None:
        """Handle conversation updates for both user and agent messages."""
        if not self.conversation_id:
            return

        # Store message in conversation history
        self.conversation_history.append(
            LLMMessage(role=event.item.role, content=event.item.text_content)
        )

        # For LLM responses, store additional metadata and update metrics
        if event.item.role == "assistant":
            # Get token counts for metrics
            total_tokens = len(event.item.text_content.split())
            
            # Update metrics
            self.metrics.llm_tokens_generated.inc(total_tokens)
            if hasattr(event.item, "latency"):
                self.metrics.llm_latency.observe(event.item.latency)
            if hasattr(event.item, "prompt"):
                self.metrics.llm_prompt_tokens.inc(len(event.item.prompt.split()))
            
            # Store in backend
            asyncio.create_task(
                self.backend_client.store_llm_response(
                    conversation_id=self.conversation_id,
                    prompt=event.item.prompt if hasattr(event.item, "prompt") else "",
                    response=event.item.text_content,
                    metadata={
                        "model": getattr(self.llm_config, "model", "unknown"),
                        "provider": getattr(self.llm_config, "provider", ProviderType.UNKNOWN).value,
                        "interrupted": event.item.interrupted,
                        "timestamp": datetime.utcnow().isoformat()
                    }
                )
            )

    async def _handle_speech_created(self, event: "SpeechCreatedEvent") -> None:
        """Handle TTS speech generation events."""
        if not self.conversation_id:
            return

        # Update metrics
        self.metrics.tts_requests.inc()
        text = event.speech_handle.text if hasattr(event.speech_handle, "text") else ""
        self.metrics.tts_characters.inc(len(text))
        
        if hasattr(event, "latency"):
            self.metrics.tts_latency.observe(event.latency)
            
        if hasattr(event.speech_handle, "duration_ms"):
            self.metrics.tts_audio_duration.observe(event.speech_handle.duration_ms / 1000.0)

        # Store TTS metadata
        asyncio.create_task(
            self.backend_client.store_tts_metadata(
                conversation_id=self.conversation_id,
                text=text,
                metadata={
                    "source": event.source,
                    "user_initiated": event.user_initiated,
                    "model": getattr(self.tts_config, "model", "unknown"),
                    "provider": getattr(self.tts_config, "provider", ProviderType.UNKNOWN).value,
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
        )

    async def _handle_agent_state(self, event: "AgentStateChangedEvent") -> None:
        """Handle agent state changes."""
        logger.info(f"Agent state changed: {event.old_state} -> {event.new_state}")
        
        # Update session duration metric
        session_duration = (datetime.utcnow() - self.session_start).total_seconds()
        self.metrics.session_duration.set(session_duration)

    async def _handle_user_state(self, event: "UserStateChangedEvent") -> None:
        """Handle user state changes."""
        logger.info(f"User state changed: {event.old_state} -> {event.new_state}")
        
        # Handle user going away
        if event.new_state == "away":
            await self.session.generate_reply(
                instructions="The user has been inactive. Ask if they're still there or need help."
            )

    async def _handle_error(self, event: "ErrorEvent") -> None:
        """Handle errors during the session."""
        logger.error(
            f"Error in {event.source}: {event.error}",
            extra={"recoverable": event.error.recoverable}
        )
        
        # Update error metrics with labels
        self.metrics.errors.inc(labels={
            "source": str(event.source),
            "recoverable": str(event.error.recoverable)
        })

        if not event.error.recoverable:
            # Inform user of the error
            await self.session.say(
                "I apologize, but I'm experiencing some technical difficulties. " 
                "Please try again in a moment."
            )
            
            # Log unrecoverable error to backend
            if self.backend_client and self.conversation_id:
                await self.backend_client.log_error(
                    room_name=self.session.room.name,
                    error_type=str(event.source),
                    error_message=str(event.error),
                    stack_trace=getattr(event.error, "stack_trace", None)
                )

    async def stt_node(
        self,
        audio: AsyncIterable[rtc.AudioFrame],
        model_settings: ModelSettings
    ) -> AsyncIterable[stt.SpeechEvent]:
        """
        Transcribes audio frames into speech events.
        Storage of transcriptions is handled by the user_input_transcribed event.
        """
        if self.stt_provider:
            # Use our custom STT provider
            async def process_audio():
                async for frame in audio:
                    # Process audio frames with custom provider
                    processed_frame = await self.stt_provider.preprocess_audio(frame)
                    yield processed_frame
            
            # Return speech events from our provider
            return await self.stt_provider.transcribe_stream(process_audio())
            
        # Fallback to default LiveKit STT if no custom provider
        return await Agent.default.stt_node(self, audio, model_settings)

    async def llm_node(
        self,
        chat_ctx: llm.ChatContext,
        tools: list[FunctionTool],
        model_settings: ModelSettings
    ) -> AsyncIterable[llm.ChatChunk]:
        """
        Performs inference based on the chat context and creates the agent's response.
        The LLM responses are now handled by the conversation_item_added event.
        """
        if self.llm_provider:
            # Use our custom LLM provider
            return self.llm_provider.generate_response(chat_ctx.messages, tools)
            
        # Fallback to default LiveKit LLM
        return await Agent.default.llm_node(self, chat_ctx, tools, model_settings)

    async def tts_node(
        self,
        text: AsyncIterable[str],
        model_settings: ModelSettings
    ) -> AsyncIterable[rtc.AudioFrame]:
        """
        Synthesizes audio from text segments.
        TTS metadata is now handled by the speech_created event.
        """
        # Collect text chunks
        text_chunks = []
        async for chunk in text:
            text_chunks.append(chunk)
        complete_text = ''.join(text_chunks)

        if self.tts_provider:
            # Use our custom TTS provider
            return self.tts_provider.synthesize_speech(complete_text)

        # Fallback to default LiveKit TTS
        return await Agent.default.tts_node(self, text, model_settings)

    async def transcription_node(
        self,
        text: AsyncIterable[str],
        model_settings: ModelSettings
    ) -> AsyncIterable[str]:
        """
        Process transcriptions before they're sent to the user.
        We can clean up formatting, fix punctuation, etc.
        """
        async for delta in text:
            # Clean up transcription if needed
            cleaned_text = delta.strip()
            if cleaned_text:
                yield cleaned_text

    async def cleanup(self):
        """Clean up resources and save final session state."""
        logger.info("Cleaning up voice assistant agent")
        
        if self.conversation_id and self.backend_client:
            # Save final conversation state
            await self.backend_client.save_conversation_state(
                conversation_id=self.conversation_id,
                messages=self.conversation_history
            )
        
        # Clean up providers
        if self.stt_provider:
            await self.stt_provider.cleanup()
        if self.llm_provider:
            await self.llm_provider.cleanup()
        if self.tts_provider:
            await self.tts_provider.cleanup()
        
        # Clean up backend client
        if self.backend_client:
            await self.backend_client.cleanup()
        
        logger.info("Cleanup complete")


async def entrypoint(ctx: JobContext):
    """
    Entry point for the LiveKit agent job.
    Handles agent initialization, room connection, and lifecycle management.
    """
    # Set up logging context
    ctx.log_context_fields = {
        "worker_id": ctx.worker_id,
        "room_name": ctx.room.name,
        "job_id": ctx.job.id
    }
    
    logger.info("Initializing agent job", extra=ctx.log_context_fields)
    
    # Load settings and create backend client
    settings = Settings()
    backend_client = BackendClient(settings)
    
    # Load job metadata if provided
    if ctx.job.metadata:
        try:
            metadata = json.loads(ctx.job.metadata)
            logger.info(f"Job metadata loaded: {metadata}", extra=ctx.log_context_fields)
        except json.JSONDecodeError:
            logger.warning("Failed to parse job metadata", extra=ctx.log_context_fields)
            metadata = {}
    else:
        metadata = {}
    
    # Create provider configs
    def get_provider_type(provider_string: str) -> ProviderType:
        """Safely convert provider string to ProviderType enum."""
        try:
            return ProviderType[provider_string.upper()]
        except (KeyError, AttributeError):
            logger.warning(f"Invalid provider type: {provider_string}, falling back to OPENAI")
            return ProviderType.OPENAI

    stt_config = STTConfig(
        provider=get_provider_type(settings.stt_provider),
        model=settings.stt_model,
        language=settings.stt_language,
        metadata={"api_key": settings.openai_api_key}
    )
    
    llm_config = LLMConfig(
        provider=get_provider_type(settings.llm_provider),
        model=settings.llm_model,
        temperature=settings.llm_temperature,
        max_tokens=settings.llm_max_tokens,
        system_prompt=settings.llm_system_prompt,
        metadata={"api_key": settings.openai_api_key}
    )
    
    tts_config = TTSConfig(
        provider=get_provider_type(settings.tts_provider),
        voice=settings.tts_voice,
        model=settings.tts_model,
        metadata={"api_key": settings.openai_api_key}
    )
    
    # Create agent instance
    agent = VoiceAssistantAgent(
        settings=settings,
        backend_client=backend_client,
        stt_config=stt_config,
        llm_config=llm_config,
        tts_config=tts_config
    )
    
    # Register shutdown hook for cleanup
    async def cleanup_hook():
        logger.info("Running cleanup hook", extra=ctx.log_context_fields)
        await agent.cleanup()
        await backend_client.cleanup()
    
    ctx.add_shutdown_callback(cleanup_hook)
    
    # Connect to the room
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info("Connected to room", extra=ctx.log_context_fields)
    
    try:
        # Wait for first participant
        participant = await ctx.wait_for_participant()
        logger.info(
            f"First participant joined: {participant.identity}",
            extra=ctx.log_context_fields
        )
        
        # Create conversation record in backend
        agent.conversation_id = await backend_client.agent_joined(
            room_name=ctx.room.name,
            agent_metadata={
                "stt_provider": settings.stt_provider,
                "llm_provider": settings.llm_provider,
                "tts_provider": settings.tts_provider,
                "participant": participant.identity
            }
        )
        
        # Start agent session
        session = AgentSession(
            stt=settings.stt_model,
            llm=settings.llm_model,
            tts=settings.tts_model
        )
        
        await session.start(
            room=ctx.room,
            agent=agent,
            room_input_options=RoomInputOptions(
                noise_cancellation=True
            )
        )
        
        # Keep running until all participants leave or job is terminated
        await ctx.wait_for_disconnection()
        
    except Exception as e:
        logger.error(f"Error in agent job: {str(e)}", exc_info=True, extra=ctx.log_context_fields)
        # Ensure proper cleanup on error
        ctx.shutdown(reason=f"Error occurred: {str(e)}")
    finally:
        logger.info("Agent job completed", extra=ctx.log_context_fields)


if __name__ == "__main__":
    # Run the agent
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
        )
    )
